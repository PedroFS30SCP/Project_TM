{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e19bb5f8",
   "metadata": {},
   "source": [
    "# üí¨ TMCD ‚Äì Trabalho Final\n",
    "## An√°lise de Sentimentos em Reviews de Filmes\n",
    "\n",
    "### üë• Grupo Trab-grupo-30\n",
    "- **Rafael Alexandre Dias Andorinha**, n¬∫ 131000  \n",
    "- **Pedro Fonte Santa**, n¬∫ 105306  \n",
    "\n",
    "---\n",
    "\n",
    "üìÖ **Data de entrega:** 26 de abril  \n",
    "\n",
    "üìä **Objetivo deste script:**\n",
    "Este Notebook corresponde √† Tarefa 2.5 do trabalho.\n",
    "\n",
    "O objetivo desta etapa √© aplicar um modelo pr√©-treinado baseado em transformadores ao problema de an√°lise de sentimentos no dataset IMDB. A tarefa √© dividida em duas fases:\n",
    "\n",
    "- **Etapa a)**: aplicar modelos pr√©-treinados diretamente, usando pipelines da biblioteca Hugging Face.\n",
    "- **Etapa b)**: realizar fine-tuning de um modelo pr√©-treinado, ajustando-o aos dados espec√≠ficos do projeto.\n",
    "\n",
    "O modelo escolhido foi o `distilbert-base-uncased-finetuned-sst-2-english` para a primeira etapa, e `distilbert-base-uncased` para o fine-tuning. A implementa√ß√£o foi baseada no notebook da aula \"BERT-based classification\", adaptado ao contexto do trabalho.\n",
    "\n",
    "---\n",
    "\n",
    "# üóÇÔ∏è Dataset: IMDB Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8478a878",
   "metadata": {},
   "source": [
    "### üìò Etapa a) ‚Äî Pipeline direto com Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "547b96a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate --quiet\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4415e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f313fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../dataset/imdb_reviews_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58ec7fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "clf = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    framework=\"pt\"  # <-- for√ßar a usar PyTorch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e928ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 688/688 [2:23:02<00:00, 12.47s/it]  \n"
     ]
    }
   ],
   "source": [
    "texts = df_test['text'].tolist()\n",
    "batch_size = 32\n",
    "preds = []\n",
    "\n",
    "# Processar por batches com barra de progresso\n",
    "for i in tqdm(range(0, len(texts), batch_size)):\n",
    "    batch = texts[i:i+batch_size]\n",
    "    preds.extend(clf(batch, truncation=True))\n",
    "\n",
    "df_test['pred_pipeline'] = [p['label'].lower() for p in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28d57f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter as predi√ß√µes do pipeline para o mesmo formato dos r√≥tulos\n",
    "df_test['pred_pipeline'] = df_test['pred_pipeline'].map({'positive': 'pos', 'negative': 'neg'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d1700f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos' 'neg']\n",
      "['positive' 'negative']\n"
     ]
    }
   ],
   "source": [
    "print(df_test['label'].unique())\n",
    "print(df_test['pred_pipeline'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c253c09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.89      0.92      0.90     11050\n",
      "         pos       0.91      0.89      0.90     10946\n",
      "\n",
      "    accuracy                           0.90     21996\n",
      "   macro avg       0.90      0.90      0.90     21996\n",
      "weighted avg       0.90      0.90      0.90     21996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(\n",
    "    df_test['label'],\n",
    "    df_test['pred_pipeline'],\n",
    "    target_names=['neg', 'pos']\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc13ddf",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Etapa b) ‚Äì Fine-tuning com Hugging Face Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610738de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall transformers -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a59b0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.51.3 --force-reinstall --no-cache-dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "479280c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp312-cp312-macosx_10_9_x86_64.whl.metadata (61 kB)\n",
      "Downloading numpy-1.26.4-cp312-cp312-macosx_10_9_x86_64.whl (20.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.5\n",
      "    Uninstalling numpy-2.2.5:\n",
      "      Successfully uninstalled numpy-2.2.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 3.5.0 requires fsspec[http]<=2024.12.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.26.4 --force-reinstall --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318948d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets evaluate --quiet\n",
    "!pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9d10099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python path: /usr/local/bin/python3\n",
      "Transformers version: 4.51.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python path:\", sys.executable)\n",
    "\n",
    "import transformers\n",
    "print(\"Transformers version:\", transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5160925a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-24 16:43:04.643681: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23da9f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar os ficheiros CSV\n",
    "df_train = pd.read_csv('../dataset/imdb_reviews_train.csv')\n",
    "df_test = pd.read_csv('../dataset/imdb_reviews_test.csv')\n",
    "\n",
    "# Mapear labels para 0 e 1 se necess√°rio (s√≥ se n√£o estiverem j√° assim)\n",
    "label_map = {'neg': 0, 'pos': 1}\n",
    "df_train['label'] = df_train['label'].map(label_map)\n",
    "df_test['label'] = df_test['label'].map(label_map)\n",
    "\n",
    "# Converter para Hugging Face datasets\n",
    "train_ds = Dataset.from_pandas(df_train[['text', 'label']])\n",
    "test_ds = Dataset.from_pandas(df_test[['text', 'label']])\n",
    "\n",
    "dataset = DatasetDict({'train': train_ds, 'test': test_ds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e75d03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21754/21754 [00:27<00:00, 782.88 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21996/21996 [00:35<00:00, 618.27 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_train = dataset[\"train\"].map(preprocess_function, batched=True)\n",
    "tokenized_test = dataset[\"test\"].map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "255e9801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85e565b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "        \"precision\": precision.compute(predictions=predictions, references=labels, average='binary')[\"precision\"],\n",
    "        \"recall\": recall.compute(predictions=predictions, references=labels, average='binary')[\"recall\"],\n",
    "        \"f1\": f1.compute(predictions=predictions, references=labels, average='binary')[\"f1\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5994e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e54ead",
   "metadata": {},
   "source": [
    "### üöÄ Treinar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eb1963",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3ce5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_finetuned = trainer.evaluate()\n",
    "print(results_finetuned)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
